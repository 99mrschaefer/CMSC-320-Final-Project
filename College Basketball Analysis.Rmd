---
title: "College Basketball Analysis"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
output: html_document
---

#### By Matt Schaefer - (Solo Project)

## 1. Introduction

Every spring, (those occurring during pandemics excepted), millions of Americans draw up brackets and bet on
the success and failure of 64 college basketball teams in a postseason finale known as March Madness. No one 
has ever submitted a perfect bracket, and the odds of it happening are 2.4 trillion to one according to a 
<a href="https://math.duke.edu/news/duke-math-professor-says-odds-perfect-bracket-are-one-24-trillion">Duke math
professor^1^</a>.

Sure, it's highly unlikely even the best mathematical model will allow a person to flawless predict the outcome
of the entire tournament. But what if we could build a model that would improve predictions enough to win
a pool of brackets - the type so many people compete in every March.

This notebook will atttempt to do just that, first covering the process of tidying data found online, then doing
Exploratory Data Analysis, then finally, creating a model to predict teams' NCAA March Madness performance given 
their season record and tournament seed. We'll go over a couple different ways to build the model and compare 
their efficacy.

The steps are as follows:

1. Data cleaning
    + Read & investigate data/entities/attributes
    + Add necessary attributes
    + Adjust postseason attribute so that it is represented as an integer value
    + Remove unnecessary attributes
    + Standardize continuous attributes
    + Partition data into a training and testing set
2. Exploratory Data Analysis
    + Univariate analysis
    + Bivariate analysis
    + Multivariate analysis
3. Model Building & Evaluationn
    + Decision Tree
    + Random Forest
4. Conclusions
5. Citations

### 1.1 Setting up necessary libraries

Here, we include some helpful R libraries designed to make data science and data representation easy.
The comments beside each library gives a basic description of what each is for. For more info on R libraries and 
packages, click <a href=https://support.rstudio.com/hc/en-us/articles/201057987-Quick-list-of-useful-R-packages>
here^2^</a>. However, before using them on your own machine, remember that you need to install the package. You can 
do this in RStudio by navigating to the lower right hand panel and clicking 'Packages,' or by using the command,
install.packages("package name") in the console.

```{r setup, include=FALSE}
library(tidyverse) # data manipulation
library(broom) # working with tidy data
library(stringr) # string manipulation + data cleaning
library(ggplot2) # data vizualization
library(dplyr) # data manipulation
library(readr) # csv reading
library(tidyr) # creating tidy data
library(magrittr) # working with pipelines
library(vioplot) # violin plots
library(rpart) # decision trees
library(rpart.plot) # plotting decision trees
library(randomForest) # random forest
library(cvTools) # cross validation
```

### 1.2 Reading in data and evaluating its organization

Now that we are ready to work, we want to access our data and evaluate its organization. We use the read_csv 
function to read the data which we've downloaded via csv file, when the glimpse function to get a quick look
at how the data is organized.
```{r initialize dataframe}
bball_tab <- read_csv("~/CMSC320-Final-Project/cbb.csv", col_types = cols()) # read in data
glimpse(bball_tab) # observe entity and attribute types
```
We want to see whether the data is tidy. This means that each row contains 1 entity and each column represents
1 attribute. Basically, an entity is a real life 'object' while an attribute describes such an object.
In our case, an entity is a college basketball team in a single season. Our attributes include the team's name, 
conference, and stats describing their performance. For more information on the distinction between entities and
attributes, <a href=https://binaryterms.com/difference-between-entity-and-attribute-in-database.html>here is a 
useful site^3^</a>. We see with the glimpse command that our data is already tidy. We also see that there are 
1757entities with 24 attributes each. 21 of these attributes are doubles while the remaining three are 
characters. The 'POSTSEASON' attribute is of special interest to us, since this is the aspect we are aiming to 
predict.

### 1.3 Investigate attributes

Now that we know a bit more about the data's organization, we want to make sure we understand the data's 
attributes and what they each mean. A full description can be found on <a 
href=https://www.kaggle.com/andrewsundberg/college-basketball-dataset>kaggle^4^</a>, where we downloaded
our data set, but below is a summarization. I've also included a classification for each attribute. Some 
common attribute types include text, continuous and discrete numeric, and unordered/ordered categorical. Click
<a href=https://www.geeksforgeeks.org/understanding-data-attribute-types-qualitative-and-quantitative/>here^5^
</a> for a detailed description of types of attributes and their differences.

* TEAM - team/school name (text-nominal)
* CONF - conference id (text-nominal)
* G - games played (discrete numeric)
* W - wins (discrete numeric)
* ADJOE - points scored per 100 possessions (continuous numeric)
* ADJDE - points allowed per 100 possessions (continuous numeric)
* BARTHAG - probability of beating avg. DIV I team (continuous numeric)
* EFG_O - field goal pctg (continuous numeric)
* EFG_D - defensive field goal pctg (continuous numeric)
* TOR - turnover rate (continuous numeric)
* TORD - steal rate (continuous numeric)
* ORB - offensive rebound pctg (continuous numeric)
* DRB - defensive rebound pctg (continuous numeric)
* FTR - free throw pctg (continuous numeric)
* FTRD - defensive free throw pctg (continuous numeric)
* 2P0 - 2 point shooting pctg (continuous numeric)
* 2PD - defensive 2 point shooting pctg (continuous numeric)
* 3P0 - 3 point shooting pctg (continuous numeric)
* 3PD - defensive 3 point shooting pctg (continuous numeric)
* ADJ_T - possessions per 40 minutes (continuous numeric)
* WAB - wins above necessary amount to qualify for postseason (discrete numeric)
* POSTSEASON - round of the postseason the team made it to (ordered categorical)
* SEED - tournament seed (discrete numeric)
* YEAR - season of play/tournament (discrete numeric)

### 1.4 Add attributes

In order to work with our data, we need to adjust some of the given attributes and tweak some of the data. 
Remember, our goal is to predict postseason performance. Hypothetically, before a tournament we will already
know we don't have to worry about teams that did not make the cut, so we will disregard those teams and eliminate
them from our data set (their postseason attribute is listed as NA). We also want to change our postseason 
attribute from categorical unordered (represented by a text string) to a numerical attribute, so it is easier to 
graph. Finally, we want to replace Wins and Games with a better metric with which we can compare teams, so we add
win-loss ratio. 
```{r clean data}
clean_bball <- bball_tab %>% 
  filter(!is.na(`POSTSEASON`)) %>% # filter out teams that did not make the postseason
  mutate(pstsn_rank=0) %>% # create a new numerical attribute to represent postseason performance
  mutate(win_ratio=W/(G-W)) # add a win-loss ratio as an attribute
```

Next, we take the text attribute, POSTSEASON, of the original table and assign a corresponding number to each
entity. The champion is assigned 7, the second place team 6, teams who made it to the semi finals 5, and so on.
```{r clean_data}
for (i in 1:340) {
  j<-clean_bball[i,"POSTSEASON"]
  if(j=="Champions"){
    clean_bball[i,"pstsn_rank"]<-7
  } else if(clean_bball[i,"POSTSEASON"]=="2ND"){
    clean_bball[i,"pstsn_rank"]<-6
  } else if(clean_bball[i,"POSTSEASON"]=="F4"){
    clean_bball[i,"pstsn_rank"]<-5
  } else if(clean_bball[i,"POSTSEASON"]=="E8"){
    clean_bball[i,"pstsn_rank"]<-4
  } else if(clean_bball[i,"POSTSEASON"]=="S16"){
    clean_bball[i,"pstsn_rank"]<-3
  } else if(clean_bball[i,"POSTSEASON"]=="R32"){
    clean_bball[i,"pstsn_rank"]<-2
  } else if(clean_bball[i,"POSTSEASON"]=="R64"){
    clean_bball[i,"pstsn_rank"]<-1
  } else {
    clean_bball[i,"pstsn_rank"]<-0
  }
}
```

### 1.5 Remove unnecessary attributes

We remove all attributes that do not impact our analysis, including team, conference, and year.
For simplicity's sake, we also remove attributes that intuitively are less important to the outcome of the 
tournament: EFG_O, EFG_D, TOR, TORD, ORB, DRB, FTR, FTRD, 2P0, 2PD, 3P0, 3PD, ADJ_T, and WAB.
It also includes games, wins, and postseason since we have effectively replaced these attributes.
```{r remove}
ncaa_tab <- clean_bball[,c(26,25,23,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,24)]
ncaa_tab <- subset(ncaa_tab, select = -c(TEAM,CONF,G,W,POSTSEASON,YEAR,EFG_O, EFG_D, TOR, TORD, ORB, DRB, FTR, 
                                         FTRD, ADJ_T, WAB))
ncaa_tab <- ncaa_tab[-7]
ncaa_tab <- ncaa_tab[-7]
ncaa_tab <- ncaa_tab[-7]
ncaa_tab <- ncaa_tab[-7]
```

### 1.6 Standardize continuous data

When graphing and looking for relationships in a set data, it is advantageous for it to be distributed normally.
Oftentimes, the best way to ensure data is easy to graph is by centering (setting the mean to 0) and scaling 
(setting standard devation to 1) the data. This way, it becomes fairly evenly distributed, and it also shows 
trends more clearly than when an outlier may have skewed the data range. Below, we scale and center all the 
continuous numeric attributes in our set. Click <a 
href=https://www.youtube.com/watch?time_continue=958&v=Vv69uhUiGU4&feature=emb_logo>this link^6^</a> for a video 
explaining the merits of this process (it's a bit long, though).

```{r standardize cont. data}
ncaa_tab<-ncaa_tab %>%
  mutate(std_winrat=(win_ratio-mean(win_ratio))/(sd(win_ratio))) %>%
  mutate(std_adjoe=(ADJOE-mean(ADJOE))/(sd(ADJOE))) %>%
  mutate(std_adjde=(ADJDE-mean(ADJDE))/(sd(ADJDE))) %>%
  mutate(std_barthag=(BARTHAG-mean(BARTHAG))/(sd(BARTHAG)))
```

Now on to the next phase!

## 2. Exploratory Data Analysis

For our EDA, we'll use the entire data set to get the best representation of the data. Our goal is to see 
general trends in the data and observe relationships between attributes.

We will mainly be using the ggplot2 library to do our plotting. Documentation can be found <a 
href=https://ggplot2.tidyverse.org/>here^7^</a>. We also utilize the vioplot library for violin plots. 
Documentation for this library can be found <a href=https://cran.r-project.org/web/packages/vioplot/index.html>here^8^</a>.

### 2.1 Univariate analysis
First, we look at the univariate distributions. This entails observing distributions of individual attributes.

Below we look at the univariate distribution of tournament placement. Since the data can only be
one of eight values (this attribute is discrete numerical), we will use a histogram:
```{r univar1}
ncaa_tab %>% 
  ggplot(aes(x=pstsn_rank)) + 
  geom_histogram(stat="count",fill="red")
```

Based on the way the tournament works, the data is distibuted as expected. A handful of the lower ranking teams 
are eliminated in <a href=https://ftw.usatoday.com/2018/03/march-madness-ncaa-tournament-no-11-seed-first-four-play-in-16-bracket-rules-selection>
qualifying games^9^</a>, then elimination works as a factor of $TotalData*(\frac{1}{2})^r$ where r is rank.

Next we look at distribution of seed similarly via a histogram:
```{r univar2}
ncaa_tab %>% 
  ggplot(aes(x=SEED)) + 
  geom_histogram(stat="count",fill="red")
```

For the most part, seed is uniformly distributed. Notable exceptions are 11th and 16th seed, as they compete in
the aforementioned pre-qualifying round where 68 teams are narrowed to 64.

Now, let's take a look at win-loss ratio. This data is cannot be counted the same way as the above data since
team's win loss ratios are continuous instead of discrete, so this time we'll use a density plot. 
```{r uninvar3}
ncaa_tab %>%
  ggplot(aes(x=win_ratio)) +
    geom_density()
```

We can see that data is not well centered; it seems most teams win-loss ratios are ~2, and several outliers are 
much greater than the majority of the data, thus we will use a slightly better representation, the standardized 
win-loss:
```{r uninvar4}
ncaa_tab %>%
  ggplot(aes(x=std_winrat)) +
    geom_density()
```

While outliers are still a problem, data is much better distributed.

Next, we look at offensive and defensive efficiency. We can view them on the same plot, since both use the same
metric: points (for and against) per 100 possessions. Rather than using a density plot, we'll use 
a different vizualiation which allows us to see the median directly as well as the distribution of 
data and the presence of outliers - violin plots:
```{r univar5}
vioplot(ncaa_tab$ADJOE, ncaa_tab$ADJDE, names=c("Offensive Eff.","Defensive Eff."))
title("Violin Plots of Offensive and Defensive Efficiency (pts./100 possessions)")
```

Outliers would show up on violin plots as points off to the ends of the violin shapes.
These violin plots show that teams are distributed very normally by their offensive/defensive efficiency, with 
little to no skew since there are no outliers visible. The shape and positioning of the white circle shows a 
clear concentration of data at the median. The plots also shows that teams are generally more efficient 
offensively relative to defensively. This makes sense, since the teams in our dataset performed highly relative 
to the rest of the nation in order to qualify for the tournament, implying that a majority of the time, they 
must score more points per 100 possessions than are scored against them. <a 
href=https://mode.com/blog/violin-plot-examples/>Here^10^</a> is a helpful resource which gives a bit more 
detail on violin plots.

While the standardized data does not show the disparity, it will make it easier to compare entities. Thus, below
is the corresponding violin plot:
```{r univar6}
vioplot(ncaa_tab$std_adjoe, ncaa_tab$std_adjde, names=c("Offensive Eff.","Defensive Eff."))
title("Violin Plots of Standardized Offensive and Defensive Efficiency")
```

We can observe that both data sets are centered at zero and distributed fairly evenly (since std dev = 1).

Lastly, we observe BARTHAG, or teams' probability of beating an average Division I team:
```{r univar7}
vioplot(ncaa_tab$BARTHAG, names=c("BARTHAG"))
title("Violin Plot of Prob. of Beating Avg. Division I Team (BARTHAG)")
```

Again, we see that there are no outliers, however, based on the long tail in the negative direction, we can see 
that this data is skewed negatively. As we might guess, (since this is a collection of the best ranked 
teams), the data is clustered above .8, showing how a sizeable majority of teams in the tournament have a very 
good chance of defeating an average DI team.

To work with this stat better, let's look at the standardized version:
```{r univar8}
vioplot(ncaa_tab$std_barthag, names=c("BARTHAG"))
title("Violin Plot of Standardized BARTHAG")
```

The data is now centered nearer to zero, however, the negative skew still prevents a normal distribution.

### 2.2 Bivariate analysis

Now, we'll look at how each variable impacts team's postseason performance by plotting postseason rank on the 
y axis and the other attributes on the x axis.

Here is the relationship between seed and postseason performance using a scatter plot. We also use the 
geom_smooth() command with the linear methond (lm) to approximate a linear relationship:
```{r seed and post}
ncaa_tab %>%
  ggplot(aes(x=SEED,y=pstsn_rank)) +
  geom_point() + 
  geom_smooth(method=lm)
```

This graph and the linear approximation shows us that the lower number a seed is, the better they perform in the 
tournament. This is intuitive, since better ranked teams are seeded lower.

Now we observe how win-loss rate affects postseason performance similarly using a scatter plot and a linear
approximation:
```{r winrat and post}
ncaa_tab %>%
  ggplot(aes(x=std_winrat,y=pstsn_rank)) +
  geom_point() + 
  geom_smooth(method=lm)
```

We repeat for barthag:
```{r barthag and post}
ncaa_tab %>%
  ggplot(aes(x=std_barthag,y=pstsn_rank)) +
  geom_point() + 
  geom_smooth(method=lm)
```

Again, our linear relationship is intuitive, as one would expect a higher chance of defeating an average team
would correlate with a better postseason performance.

We repeat for offensive efficiency:
```{r off and post}
ncaa_tab %>%
  ggplot(aes(x=std_adjoe,y=pstsn_rank)) +
  geom_point() + 
  geom_smooth(method=lm)
```

Again, our result makes sense since a more efficient offense will allow a team to perfom better in the 
tournament.

We repeat again for defensive efficiency:
```{r def and post}
ncaa_tab %>%
  ggplot(aes(x=std_adjde,y=pstsn_rank)) +
  geom_point() + 
  geom_smooth(method=lm)
```

Yet again, the relationship makes sense since fewer points against on average will allow a team to perform 
better in the tournament.

### 2.3 Multivariate analysis

For this, we will observe how seeding affects the relationships between other attributes and postseason 
performance.

Note that the general trend observed in the bivariate analysis above showed that a lower seed corresponded with 
a better performance in the tournament on average.

For this section, we'll repeat the scatter plots used above omitting the one corresponding to seed, this time
incorporating the seed attribute into each plot using the color option in ggplot2.

This allows us to see the conditional relationship between postseason performance and each attribute conditioning
on their seed. <a href=https://www.mathsisfun.com/data/probability-events-conditional.html>This resource^11^</a> 
gives more information on conditional probabilities.

Below, we see that teams with high win-loss rate which have better postseason records are generally overlap with 
the teams have a low seed and vice versa.
```{r winrat and post w seed}
ncaa_tab %>%
  ggplot(aes(x=std_winrat,y=pstsn_rank, color=SEED)) +
  geom_point() + 
  geom_smooth(method=lm)
```

The same convergent trend can be seen here:
```{r barthag and post w seed}
ncaa_tab %>%
  ggplot(aes(x=std_barthag,y=pstsn_rank, color=SEED)) +
  geom_point() + 
  geom_smooth(method=lm)
```

We see that a better chance of beating an avg. Div I team corresponds with better postseason performance, and 
the improved performance corresponds with the teams with lower seeds.

Below we again observe the same pattern regarding offensive efficiency:
```{r off and post w seed}
ncaa_tab %>%
  ggplot(aes(x=std_adjoe,y=pstsn_rank, color=SEED)) +
  geom_point() + 
  geom_smooth(method=lm)
```

And again with defensive efficiency:
```{r def and post w seed}
ncaa_tab %>%
  ggplot(aes(x=std_adjde,y=pstsn_rank, color=SEED)) +
  geom_point() + 
  geom_smooth(method=lm)
```

Based on the bivariate and multivariate analyses, it's clear that there is correlation between each attribute 
and postseason performance. However, it's not clear whether these attributes independently influence postseason 
performance or whether some cause others leading all of them to show correlation. Based on these findings, our
model will take all attributes (seed, win ratio, offensive efficiency, defensive efficiency and BARTHAG) into 
account

## 3. Model Building & Evaluation

Because we are trying to predict the tournament round a team will reach (0-7, 0 being a pre-tourney elimination
and 7 being a championship), we cannot use linear regression, since our goal is predicting a categorical 
attribute. Thus, the two models we will explore will be a decision trees and a random forest.

From now on, we'll use our standardized data, so we drop our unstandardized data:
```{r}
ncaa_tab <- subset(ncaa_tab, select = -c(ADJOE,ADJDE,BARTHAG,win_ratio)) # dropping columns
```

The first classification we will use is a decision tree. As explained in a <a href=https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052>Towards Data Science article^12^</a>,
a decision tree is "drawn upside-down with its root at the top." Essentially, the structure represents the 
model's prediction algorithm. At every node there is a condition, and the predicted outcome is found by going to 
the leaf that corresponds to the entity's data. There are two main types of decision trees: classification and 
regression trees. Classification tree aim to predict categorical attributes, whereas we are predicting a 
numerical attribute, so the type we use in this case is a regression tree. Essentially, it takes  several 
attributes into account and partitions the data in ways that split the data into significant chunks by set 
values of the attributes. After a tree is synthesized, it often needs to be 'pruned.' This refers to the process
of discarding the negligibly important decisions from the tree.

Advantages of decision trees include easy vizualization, implicitly selecting which features to include in
predictions, compatability with numerical and categorical data, and ability to work with non-linear 
relationships. Disadvantages include potentially overfitting data, instability, and biases due to a select few 
classes taking up most of the decisions.

Here is code for and example of the type of decision tree we will be using, as well as a visual representation 
of it using all of our available data:
```{r dec tree}
tree<-rpart(pstsn_rank~SEED+std_winrat+std_adjoe+std_adjde+std_barthag,data=ncaa_tab) # create tree, mapping
# all other attributes to predict pstsn_rank, use ncaa_tab data
prune.rpart(tree=tree,cp=NULL) # prune less important decisions off of tree
rpart.plot(tree) # plot the tree below
```

As you can see in the visualization, the tree uses significant cutoff points found in the attributes fed into the
tree in order to reach a conclusion about the predicted postseason rank of any given team.

The other model we will be evaluating is the random forest. This type of model, described generally in <a 
href=https://medium.com/@Synced/how-random-forest-algorithm-works-in-machine-learning-3c0fe15b6674^13^> this Medium article</a>, generates many trees using different combinations of the data. The random forest also uses a 
random selection of the attributes. In this way, it avoids the potential bias of a decision tree. However, due to
its complex synthesis of many trees, it cannot be visually represented as easily. For a more in-depth description
of the intricacies of the random forest model, Berkely has a <a 
href=https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm>really detailed page^14^</a>.

Here is an example of what the code to generate a random forest would look like:
```{r rand forest}
rand_forest <- randomForest(pstsn_rank~SEED+std_winrat+std_adjoe+std_adjde+std_barthag, importance=TRUE, mtry=3, data=ncaa_tab) # create random forest predicting postseason performance using all other attributes
```

As we said, the random forest sacrifices interpretability. However, the importance plot allows us to see which
attributes are most integral to our prediction. Below is our code to get the importance plot followed by the 
plot itself.
```{r importance plot}
varImpPlot(rand_forest) # plot importance of rand forest
```

From this plot, we can see that barthag and SEED are the attributes emphasized by our model the most.

Now that we have covered the basics of both of our classification models, our next step is to compare the 
efficacy of the two. In order to do this, we will use k-fold cross validation. 

The idea behind this is based on developing a model using one dataset then testing the efficacy on a separate 
data set, as described on <a href=https://machinelearningmastery.com/difference-test-validation-datasets/>
this^15^</a> Machine Learning Mastery page. This is necessary, since the model will be fitted to the data it is 
trained with, so you can't test it with the same data.

What k-fold cross validation does is it repeats this process k times by dividing the full dataset into 
k partitions and using one partition as a testing set at a time while using the remaining sets to train the 
models. <a href=https://machinelearningmastery.com/k-fold-cross-validation/>This site^16^</a> gives a really thorough overview of the process.

Below, we go about the process of running 10-fold cross validation to compare a decision tree and a random 
forest.
```{r}
set.seed(1234) # set seed to make partitions reproducible

fold_indices <- cvFolds(n=nrow(ncaa_tab), K=10) # get indices where data is partitioned

error_rates_bytwo <- sapply(1:10, function(fold_index) { # this function loads the error stats into a dataframe
  test_indices <- which(fold_indices$which == fold_index)
  test_set <- ncaa_tab[test_indices,] # this defines the test set
  train_set <- ncaa_tab[-test_indices,] # this defines the training set
  
  forest = randomForest(x = train_set[-1], y = train_set$pstsn_rank,
                          ntree = 500, random_state = 0) # This code trains a 500 tree random forest training all
                          # attributes to predict pstsn_rank using training data
  forest_pred = predict(forest, newdata = test_set[-1], type="response") # this generates the predictions of
                          # postseason rank based on the test data
  forest_error <- mean(abs(test_set$pstsn_rank - round(forest_pred))>=2) # this calculates the proportion of   
                          # errors per prediction for the current fold giving a 1 off leeway by defining an 
                          # error as when a prediction is off by more than 1
  
  tree <- rpart(pstsn_rank~SEED+std_winrat+std_adjoe+std_adjde+std_barthag, data=train_set) # This code trains a 
                          # decision tree to use all attributes to predict pstsn_rank using training data
  tree_pred <- predict(tree, newdata=test_set, type="vector") # generates predictions based on test data
  tree_error <-  mean(abs(test_set$pstsn_rank - round(tree_pred))>=2) # this calculates the proportion of   
                          # errors per prediction for the current fold giving a 1 off leeway by defining an 
                          # error as when a prediction is off by more than 1
  forest_error1 <- mean(test_set$pstsn_rank != round(forest_pred)) # this calculates error as strictly whether
                          # prediction does not equal actual
  tree_error1 <-  mean(test_set$pstsn_rank != round(tree_pred)) # this calculates error as strictly whether
                          # prediction does not equal actual
  c(forest_error, tree_error,forest_error1,tree_error1) # 
  })

rownames(error_rates_bytwo) <- c("forest", "tree","forest1","tree1")
error_rates_bytwo <- as.data.frame(t(error_rates_bytwo)) # tabulate data

error_rates_two <- error_rates_bytwo %>% # add fold column and finalize dataframe
  mutate(fold=1:n()) %>%
  gather(method,error,-fold)

error_rates_one <- error_rates_two[21:40,] # split off error rate where error is when pred != outcome
error_rates_twoo <- error_rates_two[1:20,] # split off error rate where error is when pred is more than one off
```

Below is the table comparing the forest and tree models when an error is defined as when the predicted postseason
performance is the same as the outcome
```{r}
error_rates_one %>% # display error data where error is when pred != outcome
  head(20) %>%
  knitr::kable("html")
```

Based on the above table, we can see that both models have similar accuracy. When an error is defined as the 
prediction being incorrect, both models predict incorrectly around ~50% of the time.

The table below comparing the forest and tree models when an error is defined as when the predicted postseason
performance is one off or less relative to the outcome
```{r}
error_rates_twoo %>% # display error data where error is when pred is more than one off
  head(20) %>%
  knitr::kable("html")
```

As might be expected, the more lax error definition results in lower error for both models. Again, neither model
seems to perform significantly better. This time, error rate is around ~10%

```{r dotplot}
dotplot(error~method, data=error_rates_two, ylab="Mean Prediction Error") # plot both defns of error for each                                                                              # model
```

This dot plot solidifies our above conclusions. Neither model is significantly better. Additionally, they are
both mediocre when it comes to exactly predicting tournament placement. However, around 90% of the time they 
can predict at an accuracy of +/-1.
```{r rates}
lm(error~method, data=error_rates_two) %>% # show table of error data
  tidy() %>%
  knitr::kable()
```

This table reinforces the conclusions we reached based on the dot plot.

## 4. Conclusions

We originally set out to create models which could boost a person's likelihood of drawing up a successful March
Madness bracket. We tidied our data, creating a data set where each entity was a college basketball team in a 
single season and kept the most relevant attributes, tweaking them to different data types when convenient.
We used data vizualization to explore the relationships between attributes and found that there was linear
correlation between all of our attributes and postseason performance. We then used cross validation to compare
the accuracy of two classifications models: a decision tree and a random forest.

Based on our findings, neither model would allow us to pick a flawless bracket. However, our goal was to find a 
model that could help us build a __better__ bracket, not a perfect one. This is why I chose to also evaluate how
the models performed when a correct choice includes being off by a single round. In this case, both models' 
performances become much better. Thus, while neither model is significantly different from the other, both have 
the potential to be useful tools when drawing up brackets, since they can certainly help narrow down which rounds
a team may advance to. 

## 5. Citations

[1] “Department of Mathematics.” [Online]. Available: https://math.duke.edu/news/duke-math-professor-says-odds-perfect-bracket-are-one-24-trillion. [Accessed: 18-May-2020].

[2] G. Grolemund, “Quick list of useful R packages,” RStudio Support, 11-May-2020. [Online]. Available: https://support.rstudio.com/hc/en-us/articles/201057987-Quick-list-of-useful-R-packages. [Accessed: 18-May-2020].

[3] N. T, “Difference Between Entity and Attribute in Database,” Binary Terms, 06-Apr-2020. [Online]. Available: https://binaryterms.com/difference-between-entity-and-attribute-in-database.html. [Accessed: 18-May-2020].

[4] A. Sundberg, “NCAA Data at https://www.kaggle.com/andrewsundberg/college-basketball-dataset.” Mar-2020.

[5] Mohityadav, “Understanding Data Attribute Types: Qualitative and Quantitative,” GeeksforGeeks, 06-Apr-2018. [Online]. Available: https://www.geeksforgeeks.org/understanding-data-attribute-types-qualitative-and-quantitative/. [Accessed: 18-May-2020].

[6] Centering and Scaling; When and when not? Sartorius Stedim Data Analytics, 2017.

[7] H. Wickham, “Create Elegant Data Visualisations Using the Grammar of Graphics,” Create Elegant Data Visualisations Using the Grammar of Graphics • ggplot2, 2016. [Online]. Available: https://ggplot2.tidyverse.org/. [Accessed: 18-May-2020].

[8] S. T. Kelly, “Violin Plot [R package vioplot version 0.3.4],” The Comprehensive R Archive Network, 29-Nov-2019. [Online]. Available: https://cran.r-project.org/web/packages/vioplot/index.html. [Accessed: 18-May-2020].

[9] M. R. Martinelli, “FTW Explains: Why do No. 11 seeds play in the First Four round of the NCAA tournament?,” USA Today, 13-Mar-2018. [Online]. Available: https://ftw.usatoday.com/2018/03/march-madness-ncaa-tournament-no-11-seed-first-four-play-in-16-bracket-rules-selection. [Accessed: 18-May-2020].

[10] J. Carron, “Violin Plots 101: Visualizing Distribution and Probability Density,” Violin Plots 101: Visualizing Distribution and Probability Density, 26-Oct-2016. [Online]. Available: https://mode.com/blog/violin-plot-examples/. [Accessed: 18-May-2020].

[11] “Conditional Probability,” Math Is Fun, 2017. [Online]. Available: https://www.mathsisfun.com/data/probability-events-conditional.html. [Accessed: 18-May-2020].

[12] P. Gupta, “Decision Trees in Machine Learning,” Medium, 12-Nov-2017. [Online]. Available: https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052. [Accessed: 18-May-2020].

[13] Synced, “How Random Forest Algorithm Works in Machine Learning,” Medium, 25-Jun-2018. [Online]. Available: https://medium.com/@Synced/how-random-forest-algorithm-works-in-machine-learning-3c0fe15b6674. [Accessed: 18-May-2020].

[14] L. Breiman and A. Cutler, “Random Forests,” Random forests - classification description. [Online]. Available: https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm. [Accessed: 18-May-2020].

[15] J. Brownlee, “What is the Difference Between Test and Validation Datasets?,” Machine Learning Mastery, 26-Jul-2017. [Online]. Available: https://machinelearningmastery.com/difference-test-validation-datasets. [Accessed: 18-May-2020].

[16] J. Brownlee, “A Gentle Introduction to k-fold Cross-Validation,” Machine Learning Mastery, 08-Aug-2019. [Online]. Available: https://machinelearningmastery.com/k-fold-cross-validation/. [Accessed: 18-May-2020].
